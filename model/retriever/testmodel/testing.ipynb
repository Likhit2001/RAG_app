{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/likhit/Documents/Coding/NLP/NLP_basics/nlp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking and encoding contexts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2400/2400 [00:01<00:00, 1786.28it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 902/902 [00:43<00:00, 20.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " chunked FAISS index and mapping saved to 'chunks_test_faiss_store/'\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import faiss\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import textwrap\n",
    "\n",
    "# Load test data\n",
    "test_data = load_dataset(\"neural-bridge/rag-dataset-12000\", split=\"test\")\n",
    "test_contexts = test_data[\"context\"]\n",
    "chunk_size = 300  # Approx characters per chunk (or use token-based)\n",
    "\n",
    "# Load model\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Storage\n",
    "os.makedirs(\"chunks_test_faiss_store\", exist_ok=True)\n",
    "all_chunks = []\n",
    "chunk_text_to_original = []\n",
    "\n",
    "# Chunk all contexts\n",
    "print(\"Chunking and encoding contexts...\")\n",
    "for original_context in tqdm(test_contexts):\n",
    "    if not isinstance(original_context, str):\n",
    "        continue\n",
    "    # Simple char-based chunking (or use token-based for better accuracy)\n",
    "    chunks = textwrap.wrap(original_context, width=chunk_size, break_long_words=False)\n",
    "    all_chunks.extend(chunks)\n",
    "    chunk_text_to_original.extend([original_context] * len(chunks))\n",
    "\n",
    "# Encode all chunks\n",
    "batch_size = 32\n",
    "context_embeddings = []\n",
    "\n",
    "for i in tqdm(range(0, len(all_chunks), batch_size)):\n",
    "    batch = all_chunks[i:i + batch_size]\n",
    "    emb = model.encode(batch, convert_to_numpy=True, show_progress_bar=False)\n",
    "    context_embeddings.append(emb)\n",
    "\n",
    "context_embeddings = np.vstack(context_embeddings)\n",
    "\n",
    "# Build and save FAISS index\n",
    "dimension = context_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(context_embeddings)\n",
    "faiss.write_index(index, \"chunks_test_faiss_store/context_index.faiss\")\n",
    "\n",
    "# Save chunk mapping\n",
    "pd.DataFrame({\n",
    "    \"chunk\": all_chunks,\n",
    "    \"original_context\": chunk_text_to_original\n",
    "}).to_csv(\"chunks_test_faiss_store/context_mapping.csv\", index=False)\n",
    "\n",
    "print(\" chunked FAISS index and mapping saved to 'chunks_test_faiss_store/'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2399/2399 [00:36<00:00, 66.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Evaluation Metrics: {'Precision@3': 0.7648, 'Recall@3': 0.9387, 'Accuracy@3': 0.9387}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load fine-tuned or base model\n",
    "model = SentenceTransformer(\"/Users/likhit/Desktop/Projects/RAG/1fineeeeminilm_proj512_only_dense\")\n",
    "\n",
    "# Load test dataset\n",
    "test_data = load_dataset(\"neural-bridge/rag-dataset-12000\", split=\"test\")\n",
    "test_questions = test_data[\"question\"]\n",
    "test_contexts = test_data[\"context\"]\n",
    "\n",
    "# Filter invalid entries\n",
    "filtered = [(q, c) for q, c in zip(test_questions, test_contexts)\n",
    "            if isinstance(q, str) and q.strip() and isinstance(c, str) and c.strip()]\n",
    "test_questions, test_contexts = zip(*filtered)\n",
    "\n",
    "# Load FAISS index and chunk-to-original mapping\n",
    "index = faiss.read_index(\"/Users/likhit/Desktop/Projects/RAG/model/retriever/testmodel/chunks_test_faiss_store/context_index.faiss\")\n",
    "context_df = pd.read_csv(\"/Users/likhit/Desktop/Projects/RAG/model/retriever/testmodel/chunks_test_faiss_store/context_mapping.csv\")\n",
    "\n",
    "# context_mapping.csv must have at least two columns: \"chunk\", \"original_context\"\n",
    "if \"original_context\" not in context_df.columns:\n",
    "    raise ValueError(\"context_mapping.csv must include a column named 'original_context'.\")\n",
    "\n",
    "# Compute evaluation metrics\n",
    "def compute_metrics(k=5):\n",
    "    correct = 0\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    total = len(test_questions)\n",
    "\n",
    "    for question, true_context in tqdm(zip(test_questions, test_contexts), total=total):\n",
    "        query_vec = model.encode([question], convert_to_numpy=True)\n",
    "        query_vec = np.expand_dims(query_vec, axis=0) if query_vec.ndim == 1 else query_vec\n",
    "\n",
    "        _, indices = index.search(query_vec, k)\n",
    "        retrieved_originals = [\n",
    "            context_df.iloc[i][\"original_context\"].strip() for i in indices[0]\n",
    "        ]\n",
    "\n",
    "        match = [1 if true_context.strip() == r else 0 for r in retrieved_originals]\n",
    "\n",
    "        precision = sum(match) / k\n",
    "        recall = 1.0 if any(match) else 0.0\n",
    "        accuracy = 1 if any(match) else 0\n",
    "\n",
    "        precision_scores.append(precision)\n",
    "        recall_scores.append(recall)\n",
    "        correct += accuracy\n",
    "\n",
    "    return {\n",
    "        f\"Precision@{k}\": round(np.mean(precision_scores), 4),\n",
    "        f\"Recall@{k}\": round(np.mean(recall_scores), 4),\n",
    "        f\"Accuracy@{k}\": round(correct / total, 4)\n",
    "    }\n",
    "\n",
    "# Run evaluation\n",
    "metrics = compute_metrics(k=3)\n",
    "print(\"ðŸ“Š Evaluation Metrics:\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForFeatureExtraction(\n",
       "  (base_model): LoraModel(\n",
       "    (model): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(30522, 384, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 384)\n",
       "        (token_type_embeddings): Embedding(2, 384)\n",
       "        (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-5): 6 x BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSdpaSelfAttention(\n",
       "                (query): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                    (LORA): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=384, out_features=8, bias=False)\n",
       "                    (LORA): Linear(in_features=384, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=384, bias=False)\n",
       "                    (LORA): Linear(in_features=8, out_features=384, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (value): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                    (LORA): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=384, out_features=8, bias=False)\n",
       "                    (LORA): Linear(in_features=384, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=384, bias=False)\n",
       "                    (LORA): Linear(in_features=8, out_features=384, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): BertPooler(\n",
       "        (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/Users/likhit/Desktop/Projects/RAG/lora_finetune_retriever\")\n",
    "\n",
    "from transformers import AutoModel\n",
    "from peft import get_peft_model, LoraConfig, PeftModel, TaskType\n",
    "import torch\n",
    "\n",
    "# Use same config as training\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.FEATURE_EXTRACTION,\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "# Load base model\n",
    "base_model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Apply LoRA\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "# Load LoRA weights\n",
    "model.load_adapter(\"/Users/likhit/Desktop/Projects/RAG/lora_finetune_retriever\" , \"LORA\")\n",
    "\n",
    "# Set to eval mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2399/2399 [00:32<00:00, 73.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Evaluation Metrics: {'Precision@5': 0.5542, 'Recall@5': 0.9033, 'Accuracy@5': 0.9033}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"âœ… Using device: {device}\")\n",
    "model.to(device)\n",
    "\n",
    "def compute_metrics2(k=5):\n",
    "    correct = 0\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    total = len(test_questions)\n",
    "\n",
    "    for question, true_context in tqdm(zip(test_questions, test_contexts), total=total):\n",
    "        # Encode with LoRA model\n",
    "        with torch.no_grad():\n",
    "            tokens = tokenizer(question, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "            q_emb = model(**tokens).last_hidden_state[:, 0].cpu().numpy()\n",
    "\n",
    "        # Ensure 2D shape for FAISS\n",
    "        q_emb = np.expand_dims(q_emb, axis=0) if q_emb.ndim == 1 else q_emb\n",
    "\n",
    "        _, indices = index.search(q_emb, k)\n",
    "\n",
    "        retrieved = [\n",
    "            context_df.iloc[i][\"original_context\"].strip() for i in indices[0]\n",
    "        ]\n",
    "\n",
    "        match = [1 if true_context.strip() == r else 0 for r in retrieved]\n",
    "\n",
    "        precision = sum(match) / k\n",
    "        recall = 1.0 if any(match) else 0.0\n",
    "        accuracy = 1 if any(match) else 0\n",
    "\n",
    "        precision_scores.append(precision)\n",
    "        recall_scores.append(recall)\n",
    "        correct += accuracy\n",
    "\n",
    "    return {\n",
    "        f\"Precision@{k}\": round(np.mean(precision_scores), 4),\n",
    "        f\"Recall@{k}\": round(np.mean(recall_scores), 4),\n",
    "        f\"Accuracy@{k}\": round(correct / total, 4)\n",
    "    }\n",
    "\n",
    "# Run evaluation\n",
    "metrics = compute_metrics2(k=5)\n",
    "print(\"ðŸ“Š Evaluation Metrics:\", metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
