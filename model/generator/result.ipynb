{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from sklearn.metrics import f1_score\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import nltk\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "retriever = SentenceTransformer(\"/Users/likhit/Desktop/Projects/RAG/1fineeeeminilm_proj512_only_dense\")\n",
    "\n",
    "\n",
    "# Load FAISS index + mapping\n",
    "index = faiss.read_index(\"/Users/likhit/Desktop/Projects/RAG/model/retriever/testmodel/chunks_test_faiss_store/context_index.faiss\")\n",
    "context_df = pd.read_csv(\"/Users/likhit/Desktop/Projects/RAG/model/retriever/testmodel/chunks_test_faiss_store/context_mapping.csv\")\n",
    "\n",
    "retriever.eval()\n",
    "retriever.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "generator = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\n",
    "generator.to(device)\n",
    "test_data = load_dataset(\"neural-bridge/rag-dataset-12000\", split=\"test\")\n",
    "test_questions = test_data[\"question\"]\n",
    "test_answers = test_data[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_top_k(question, k=5):\n",
    "    query_vec = retriever.encode([question], convert_to_numpy=True)\n",
    "    query_vec = np.expand_dims(query_vec, axis=0) if query_vec.ndim == 1 else query_vec\n",
    "\n",
    "    _, indices = index.search(query_vec, k)\n",
    "    \n",
    "    retrieved_contexts = [context_df.iloc[i][\"chunk\"] for i in indices[0]]\n",
    "    return retrieved_contexts\n",
    "\n",
    "\n",
    "def build_prompt(question, contexts):\n",
    "    prompt = f\"Answer the following question using complete sentences based only on the given context.\\n\"\n",
    "    prompt += \"\\n\".join(contexts)\n",
    "    prompt += f\"\\n\\nQuestion: {question}\\nAnswer:\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def generate_answer(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
    "    outputs = generator.generate(**inputs, max_length=512)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [10:57<00:00,  1.32s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Final Evaluation Metrics:\n",
      " {'Exact Match': 0.01, 'F1 Score': 0.2409, 'BLEU': 0.0872, 'ROUGE-L': 0.2826, 'Inclusive Accuracy': 0.354}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(k=5, max_samples=500):\n",
    "    ems, f1s, rouges, bleus, inclusives = [], [], [], [], []\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    smooth_fn = SmoothingFunction().method1\n",
    "\n",
    "    for i in tqdm(range(min(len(test_questions), max_samples))):\n",
    "        q, gt = test_questions[i], test_answers[i].strip()\n",
    "        ctxs = retrieve_top_k(q, k)\n",
    "        pred = generate_answer(build_prompt(q, ctxs)).strip()\n",
    "\n",
    "        # Normalize\n",
    "        gt_lower = gt.lower()\n",
    "        pred_lower = pred.lower()\n",
    "\n",
    "        # EM\n",
    "        em = int(gt_lower == pred_lower)\n",
    "\n",
    "        # F1\n",
    "        gt_tokens = gt_lower.split()\n",
    "        pred_tokens = pred_lower.split()\n",
    "        common = set(gt_tokens) & set(pred_tokens)\n",
    "        f1 = 2 * len(common) / (len(gt_tokens) + len(pred_tokens)) if (gt_tokens and pred_tokens) else 0.0\n",
    "\n",
    "        # BLEU\n",
    "        bleu = sentence_bleu([gt_tokens], pred_tokens, smoothing_function=smooth_fn)\n",
    "\n",
    "        # ROUGE-L\n",
    "        rouge = scorer.score(gt_lower, pred_lower)[\"rougeL\"].fmeasure\n",
    "\n",
    "        # Inclusive accuracy (substring match)\n",
    "        inclusive = int(gt_lower in pred_lower or pred_lower in gt_lower)\n",
    "\n",
    "        ems.append(em)\n",
    "        f1s.append(f1)\n",
    "        rouges.append(rouge)\n",
    "        bleus.append(bleu)\n",
    "        inclusives.append(inclusive)\n",
    "\n",
    "    return {\n",
    "        \"Exact Match\": round(np.mean(ems), 4),\n",
    "        \"F1 Score\": round(np.mean(f1s), 4),\n",
    "        \"BLEU\": round(np.mean(bleus), 4),\n",
    "        \"ROUGE-L\": round(np.mean(rouges), 4),\n",
    "        \"Inclusive Accuracy\": round(np.mean(inclusives), 4),\n",
    "    }\n",
    "\n",
    "# 7. Run evaluation\n",
    "metrics = evaluate_model(k=5, max_samples=500)\n",
    "print(\"ðŸ“Š Final Evaluation Metrics:\\n\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are they semantically same? âœ… Yes\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = \"google/flan-t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "generator = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "# Generate answer from prompt\n",
    "def generate_answer1(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = generator.generate(**inputs, max_length=64)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Use LLM to compare answer with ground truth\n",
    "def verify_equivalence(predicted, ground_truth):\n",
    "    check_prompt = (\n",
    "    f\"Is the following short answer correct based on the full answer?\\n\"\n",
    "    f\"Short answer: {predicted}\\n\"\n",
    "    f\"Full answer: {ground_truth}\\n\"\n",
    "    f\"Reply with 'Yes' or 'No'.\"\n",
    "    )\n",
    "    result = generate_answer1(check_prompt).lower()\n",
    "    return 1 if \"yes\" in result else 0\n",
    "\n",
    "pred = \"Fabien Gabel\"\n",
    "gt = \"The music director of the Quebec Symphony Orchestra is Fabien Gabel.\"\n",
    "is_equivalent = verify_equivalence(pred, gt)\n",
    "\n",
    "print(f\"Are they semantically same? {'âœ… Yes' if is_equivalent else 'âŒ No'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2400/2400 [10:49<00:00,  3.70it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Semantic Accuracy (via LLM judgment): 0.0013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# âœ… Setup model and device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = \"google/flan-t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "generator = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "# âœ… Helper: Generate LLM response\n",
    "def generate_answer1(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = generator.generate(**inputs, max_length=64)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# âœ… Helper: Use LLM to check semantic equivalence\n",
    "def verify_equivalence(predicted, ground_truth):\n",
    "    check_prompt = (\n",
    "        f\"Is the following short answer correct based on the full answer?\\n\"\n",
    "        f\"Short answer: {predicted}\\n\"\n",
    "        f\"Full answer: {ground_truth}\\n\"\n",
    "        f\"Reply with 'Yes' or 'No'.\"\n",
    "    )\n",
    "    result = generate_answer1(check_prompt).lower()\n",
    "    return 1 if \"yes\" in result else 0\n",
    "\n",
    "# âœ… Load test data (you can replace with your own predictions if available)\n",
    "dataset = load_dataset(\"neural-bridge/rag-dataset-12000\", split=\"test\")\n",
    "questions = dataset[\"question\"]\n",
    "ground_truths = dataset[\"answer\"]\n",
    "\n",
    "# â›”ï¸ Example dummy predictions â€” replace with your generated answers!\n",
    "predictions = [\"Fabien Gabel\"] * len(ground_truths)  # Replace this with actual model predictions\n",
    "\n",
    "# âœ… Loop and evaluate semantic accuracy\n",
    "correct = 0\n",
    "total = min(len(predictions), len(ground_truths))\n",
    "\n",
    "for pred, gt in tqdm(zip(predictions, ground_truths), total=total):\n",
    "    correct += verify_equivalence(pred, gt)\n",
    "\n",
    "semantic_accuracy = correct / total\n",
    "print(f\"\\nðŸ“Š Semantic Accuracy (via LLM judgment): {semantic_accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
