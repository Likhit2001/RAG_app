{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/likhit/Documents/Coding/NLP/NLP_basics/nlp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, models, losses, InputExample\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "# 🔹 Step 1: Load base transformer model (frozen)\n",
    "word_embedding_model = models.Transformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Freeze encoder layers\n",
    "for param in word_embedding_model.auto_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 🔹 Step 2: Add pooling layer\n",
    "pooling_model = models.Pooling(\n",
    "    word_embedding_model.get_word_embedding_dimension(),\n",
    "    pooling_mode_mean_tokens=True\n",
    ")\n",
    "\n",
    "# 🔹 Step 3: Add custom dense projection layers (trainable)\n",
    "dense1 = models.Dense(in_features=384, out_features=256, activation_function=nn.ReLU())\n",
    "dense2 = models.Dense(in_features=256, out_features=256, activation_function=nn.Identity())\n",
    "\n",
    "# 🔹 Step 4: Assemble the model\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model, dense1, dense2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load RAG dataset\n",
    "dataset = load_dataset(\"neural-bridge/rag-dataset-12000\", split=\"train\")\n",
    "\n",
    "# Convert to InputExample format\n",
    "train_samples = [InputExample(texts=[q, c]) for q, c in zip(dataset[\"question\"], dataset[\"context\"])]\n",
    "train_dataloader = DataLoader(train_samples, shuffle=True, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 500/6000 [04:13<42:19,  2.17it/s]                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7529, 'grad_norm': 7.112282752990723, 'learning_rate': 1.8363939899833057e-05, 'epoch': 1.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1000/6000 [08:04<38:10,  2.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3886, 'grad_norm': 6.357995510101318, 'learning_rate': 1.669449081803005e-05, 'epoch': 3.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1500/6000 [11:54<35:42,  2.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.314, 'grad_norm': 3.9594621658325195, 'learning_rate': 1.5025041736227046e-05, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 2000/6000 [15:45<30:34,  2.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2677, 'grad_norm': 6.075601577758789, 'learning_rate': 1.3355592654424041e-05, 'epoch': 6.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 2500/6000 [19:40<29:36,  1.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2484, 'grad_norm': 3.961832284927368, 'learning_rate': 1.1686143572621036e-05, 'epoch': 8.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 3000/6000 [23:51<25:05,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2218, 'grad_norm': 5.680347919464111, 'learning_rate': 1.001669449081803e-05, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 3500/6000 [27:44<19:07,  2.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2082, 'grad_norm': 5.276979923248291, 'learning_rate': 8.347245409015026e-06, 'epoch': 11.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 4000/6000 [31:35<15:37,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1934, 'grad_norm': 3.377329111099243, 'learning_rate': 6.6777963272120206e-06, 'epoch': 13.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 4170/6000 [33:00<16:55,  1.80it/s]"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m model\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      5\u001b[0m train_loss \u001b[39m=\u001b[39m losses\u001b[39m.\u001b[39mMultipleNegativesRankingLoss(model)\n\u001b[0;32m----> 7\u001b[0m model\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m      8\u001b[0m     train_objectives\u001b[39m=\u001b[39;49m[(train_dataloader, train_loss)],\n\u001b[1;32m      9\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m,\n\u001b[1;32m     10\u001b[0m     warmup_steps\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m,\n\u001b[1;32m     11\u001b[0m     show_progress_bar\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     12\u001b[0m     optimizer_params\u001b[39m=\u001b[39;49m{\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m2e-5\u001b[39;49m},\n\u001b[1;32m     13\u001b[0m     use_amp\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m  \n\u001b[1;32m     14\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/Coding/NLP/NLP_basics/nlp/lib/python3.10/site-packages/sentence_transformers/fit_mixin.py:408\u001b[0m, in \u001b[0;36mFitMixin.fit\u001b[0;34m(self, train_objectives, evaluator, epochs, steps_per_epoch, scheduler, warmup_steps, optimizer_class, optimizer_params, weight_decay, evaluation_steps, output_path, save_best_model, max_grad_norm, use_amp, callback, show_progress_bar, checkpoint_path, checkpoint_save_steps, checkpoint_save_total_limit, resume_from_checkpoint)\u001b[0m\n\u001b[1;32m    405\u001b[0m         logger\u001b[39m.\u001b[39mwarning(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCheckpoint directory does not exist or is not a directory: \u001b[39m\u001b[39m{\u001b[39;00mcheckpoint_path\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    406\u001b[0m         resume_from_checkpoint \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 408\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain(resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint)\n",
      "File \u001b[0;32m~/Documents/Coding/NLP/NLP_basics/nlp/lib/python3.10/site-packages/transformers/trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2121\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2122\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2123\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   2124\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   2125\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   2126\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   2127\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   2128\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/Coding/NLP/NLP_basics/nlp/lib/python3.10/site-packages/transformers/trainer.py:2427\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2425\u001b[0m update_step \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   2426\u001b[0m num_batches \u001b[39m=\u001b[39m args\u001b[39m.\u001b[39mgradient_accumulation_steps \u001b[39mif\u001b[39;00m update_step \u001b[39m!=\u001b[39m (total_updates \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m) \u001b[39melse\u001b[39;00m remainder\n\u001b[0;32m-> 2427\u001b[0m batch_samples, num_items_in_batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_batch_samples(epoch_iterator, num_batches)\n\u001b[1;32m   2428\u001b[0m \u001b[39mfor\u001b[39;00m i, inputs \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(batch_samples):\n\u001b[1;32m   2429\u001b[0m     step \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/Documents/Coding/NLP/NLP_basics/nlp/lib/python3.10/site-packages/transformers/trainer.py:5045\u001b[0m, in \u001b[0;36mTrainer.get_batch_samples\u001b[0;34m(self, epoch_iterator, num_batches)\u001b[0m\n\u001b[1;32m   5043\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_batches):\n\u001b[1;32m   5044\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 5045\u001b[0m         batch_samples \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [\u001b[39mnext\u001b[39;49m(epoch_iterator)]\n\u001b[1;32m   5046\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m   5047\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Coding/NLP/NLP_basics/nlp/lib/python3.10/site-packages/accelerate/data_loader.py:577\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    575\u001b[0m     current_batch \u001b[39m=\u001b[39m send_to_device(current_batch, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice, non_blocking\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_non_blocking)\n\u001b[1;32m    576\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_state_dict()\n\u001b[0;32m--> 577\u001b[0m next_batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(dataloader_iter)\n\u001b[1;32m    578\u001b[0m \u001b[39mif\u001b[39;00m batch_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mskip_batches:\n\u001b[1;32m    579\u001b[0m     \u001b[39myield\u001b[39;00m current_batch\n",
      "File \u001b[0;32m~/Documents/Coding/NLP/NLP_basics/nlp/lib/python3.10/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    702\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/Documents/Coding/NLP/NLP_basics/nlp/lib/python3.10/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/Documents/Coding/NLP/NLP_basics/nlp/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[0;32m~/Documents/Coding/NLP/NLP_basics/nlp/lib/python3.10/site-packages/sentence_transformers/data_collator.py:55\u001b[0m, in \u001b[0;36mSentenceTransformerDataCollator.__call__\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m     52\u001b[0m     batch[column_name] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([row[column_name] \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m features], dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mint)\n\u001b[1;32m     53\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m tokenized \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenize_fn([row[column_name] \u001b[39mfor\u001b[39;49;00m row \u001b[39min\u001b[39;49;00m features])\n\u001b[1;32m     56\u001b[0m \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m tokenized\u001b[39m.\u001b[39mitems():\n\u001b[1;32m     57\u001b[0m     batch[\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mcolumn_name\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m value\n",
      "File \u001b[0;32m~/Documents/Coding/NLP/NLP_basics/nlp/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:1124\u001b[0m, in \u001b[0;36mSentenceTransformer.tokenize\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m   1113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtokenize\u001b[39m(\u001b[39mself\u001b[39m, texts: \u001b[39mlist\u001b[39m[\u001b[39mstr\u001b[39m] \u001b[39m|\u001b[39m \u001b[39mlist\u001b[39m[\u001b[39mdict\u001b[39m] \u001b[39m|\u001b[39m \u001b[39mlist\u001b[39m[\u001b[39mtuple\u001b[39m[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mdict\u001b[39m[\u001b[39mstr\u001b[39m, Tensor]:\n\u001b[1;32m   1114\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m \u001b[39m    Tokenizes the texts.\u001b[39;00m\n\u001b[1;32m   1116\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1122\u001b[0m \u001b[39m            \"attention_mask\", and \"token_type_ids\".\u001b[39;00m\n\u001b[1;32m   1123\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1124\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_first_module()\u001b[39m.\u001b[39;49mtokenize(texts)\n",
      "File \u001b[0;32m~/Documents/Coding/NLP/NLP_basics/nlp/lib/python3.10/site-packages/sentence_transformers/models/Transformer.py:488\u001b[0m, in \u001b[0;36mTransformer.tokenize\u001b[0;34m(self, texts, padding)\u001b[0m\n\u001b[1;32m    486\u001b[0m batch1, batch2 \u001b[39m=\u001b[39m [], []\n\u001b[1;32m    487\u001b[0m \u001b[39mfor\u001b[39;00m text_tuple \u001b[39min\u001b[39;00m texts:\n\u001b[0;32m--> 488\u001b[0m     batch1\u001b[39m.\u001b[39mappend(text_tuple[\u001b[39m0\u001b[39;49m])\n\u001b[1;32m    489\u001b[0m     batch2\u001b[39m.\u001b[39mappend(text_tuple[\u001b[39m1\u001b[39m])\n\u001b[1;32m    490\u001b[0m to_tokenize \u001b[39m=\u001b[39m [batch1, batch2]\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "from torch import device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "train_loss = losses.MultipleNegativesRankingLoss(model)\n",
    "\n",
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    epochs=20,\n",
    "    warmup_steps=10,\n",
    "    show_progress_bar=True,\n",
    "    optimizer_params={'lr': 2e-5},\n",
    "    use_amp=False  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Reuse same contexts from earlier dataset\n",
    "contexts = dataset[\"context\"]\n",
    "\n",
    "# Encode using the trained model\n",
    "context_embeddings = model.encode(contexts, convert_to_numpy=True, batch_size=64, show_progress_bar=True)\n",
    "\n",
    "# Save mapping to find text later\n",
    "os.makedirs(\"retriever_store\", exist_ok=True)\n",
    "pd.DataFrame({\"context\": contexts}).to_csv(\"retriever_store/context_mapping.csv\", index=False)\n",
    "\n",
    "# Create and save FAISS index\n",
    "dimension = context_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(context_embeddings)\n",
    "faiss.write_index(index, \"retriever_store/context_index.faiss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import pandas as pd\n",
    "\n",
    "# Load trained model\n",
    "model = SentenceTransformer(\"output/trained_model_path\")  # or use the same model object if still in memory\n",
    "\n",
    "# Load FAISS index and context text mapping\n",
    "index = faiss.read_index(\"retriever_store/context_index.faiss\")\n",
    "context_df = pd.read_csv(\"retriever_store/context_mapping.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_top_k(query, k=1):\n",
    "    query_embedding = model.encode([query], convert_to_numpy=True)\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    results = [context_df.iloc[i][\"context\"] for i in indices[0]]\n",
    "    return results\n",
    "\n",
    "query = \"What is the Berry Export Summary 2028 and what is its purpose?\"\n",
    "top_k_contexts = retrieve_top_k(query)\n",
    "\n",
    "for i, ctx in enumerate(top_k_contexts, 1):\n",
    "    print(f\"[{i}] {ctx}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model/dpr_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DPRRetriever()  # Make sure class definition is present\n",
    "model.load_state_dict(torch.load(\"model/dpr_model.pt\", map_location=device))\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "test_data = load_dataset(\"neural-bridge/rag-dataset-12000\", split=\"test\")\n",
    "test_contexts = test_data[\"context\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Save test context embeddings\n",
    "os.makedirs(\"test_faiss_store\", exist_ok=True)\n",
    "context_embeddings = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(test_contexts), 32):\n",
    "        batch_texts = test_contexts[i:i+32]\n",
    "        embs = model.encode_passage(batch_texts, device).cpu().numpy()\n",
    "        context_embeddings.append(embs)\n",
    "\n",
    "context_embeddings = np.vstack(context_embeddings)\n",
    "\n",
    "# Save index + mapping\n",
    "pd.DataFrame({\"context\": test_contexts}).to_csv(\"test_faiss_store/context_mapping.csv\", index=False)\n",
    "dimension = context_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(context_embeddings)\n",
    "faiss.write_index(index, \"test_faiss_store/context_index.faiss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load back FAISS index + context\n",
    "index = faiss.read_index(\"test_faiss_store/context_index.faiss\")\n",
    "context_df = pd.read_csv(\"test_faiss_store/context_mapping.csv\")\n",
    "\n",
    "def retrieve_top_k(query, k=5):\n",
    "    with torch.no_grad():\n",
    "        query_vec = model.encode_query([query], device).cpu().numpy()\n",
    "    distances, indices = index.search(query_vec, k)\n",
    "    return [context_df.iloc[i][\"context\"] for i in indices[0]]\n",
    "\n",
    "# Example query\n",
    "query = \"Who won the 2021 Formula 1 championship?\"\n",
    "results = retrieve_top_k(query)\n",
    "for i, r in enumerate(results, 1):\n",
    "    print(f\"[{i}] {r}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
