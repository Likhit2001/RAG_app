{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/likhit/Documents/Coding/NLP/NLP_basics/nlp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Retriever_model(nn.Module):\n",
    "    def __init__(self):\n",
    "\n",
    "        super(Retriever_model, self).__init__()\n",
    "        self.encoder = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.pooler = nn.AdaptiveAvgPool1d(1)\n",
    "        self.dense = nn.Linear(self.encoder.config.hidden_size, 512)\n",
    "        self.dense2 = nn.Linear(512,256)\n",
    "        self.dense3 = nn.Linear(256,256)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "\n",
    "        with torch.no_grad(): \n",
    "            outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            last_hidden = outputs.last_hidden_state \n",
    "            pooled = torch.mean(last_hidden, dim=1)\n",
    "\n",
    "        pooled = F.relu(self.dense(pooled))\n",
    "        pooled = F.relu(self.dense2(pooled))\n",
    "        return self.dense3(pooled) \n",
    "\n",
    "    def encode_texts(self, texts, device):\n",
    "        encoding = self.tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        return self.forward(encoding[\"input_ids\"].to(device), encoding[\"attention_mask\"].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import Dataset\n",
    "# import torch\n",
    "# import random\n",
    "# from torch.utils.data import ConcatDataset, DataLoader\n",
    "# from datasets import load_dataset\n",
    "\n",
    "\n",
    "# data = load_dataset(\"neural-bridge/rag-dataset-12000\")\n",
    "# train_data = data.get(\"train\")\n",
    "\n",
    "\n",
    "# class Model_Dataset(Dataset):\n",
    "#     def __init__(self,data):\n",
    "#         self.data = data\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "\n",
    "#         item = self.data[index]\n",
    "#         context = item['context']\n",
    "#         question = item['question']\n",
    "\n",
    "#         return  {\n",
    "#             \"context\" : context , \n",
    "#             \"question\" : question\n",
    "#         }\n",
    "    \n",
    "\n",
    "# train_dataset = Model_Dataset(train_data)\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def multiple_negatives_ranking_loss(query_embeds, context_embeds):\n",
    "#     # [batch_size, embedding_dim] â†’ cosine sim matrix\n",
    "#     sim_scores = torch.matmul(query_embeds, context_embeds.T)  # [B, B]\n",
    "#     labels = torch.arange(len(sim_scores)).to(sim_scores.device)  # [0, 1, ..., B-1]\n",
    "#     return F.cross_entropy(sim_scores, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = Retriever_model().to(device)\n",
    "# print(device)\n",
    "\n",
    "# optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=2e-5)\n",
    "# num_epochs = 1\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     total_loss = 0.0\n",
    "\n",
    "#     for batch in train_dataloader:\n",
    "#         queries = batch[\"question\"]\n",
    "#         contexts = batch[\"context\"]\n",
    "\n",
    "#         query_embeddings = model.encode_texts(queries, device)  \n",
    "#         context_embeddings = model.encode_texts(contexts, device) \n",
    "\n",
    "#         loss = multiple_negatives_ranking_loss(query_embeddings, context_embeddings)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#     print(f\"Epoch {epoch+1} | Loss: {total_loss:.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 56\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m train_loader:\n\u001b[1;32m     55\u001b[0m     q_texts, c_texts \u001b[39m=\u001b[39m batch[\u001b[39m\"\u001b[39m\u001b[39mquestion\u001b[39m\u001b[39m\"\u001b[39m], batch[\u001b[39m\"\u001b[39m\u001b[39mcontext\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m---> 56\u001b[0m     q_embed \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mencode_texts(q_texts, device)\n\u001b[1;32m     57\u001b[0m     c_embed \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mencode_texts(c_texts, device)\n\u001b[1;32m     59\u001b[0m     loss \u001b[39m=\u001b[39m multiple_negatives_ranking_loss(q_embed, c_embed)\n",
      "Cell \u001b[0;32mIn[1], line 33\u001b[0m, in \u001b[0;36mRetriever_model.encode_texts\u001b[0;34m(self, texts, device)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mencode_texts\u001b[39m(\u001b[39mself\u001b[39m, texts, device):\n\u001b[0;32m---> 33\u001b[0m     encoding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer(texts, padding\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, truncation\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, return_tensors\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     34\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(encoding[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mto(device), encoding[\u001b[39m\"\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mto(device))\n",
      "File \u001b[0;32m~/Documents/Coding/NLP/NLP_basics/nlp/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3021\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3019\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   3020\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 3021\u001b[0m     encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_one(text\u001b[39m=\u001b[39;49mtext, text_pair\u001b[39m=\u001b[39;49mtext_pair, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mall_kwargs)\n\u001b[1;32m   3022\u001b[0m \u001b[39mif\u001b[39;00m text_target \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3023\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m~/Documents/Coding/NLP/NLP_basics/nlp/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3109\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   3104\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   3105\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbatch length of `text`: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(text)\u001b[39m}\u001b[39;00m\u001b[39m does not match batch length of `text_pair`:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3106\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(text_pair)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3107\u001b[0m         )\n\u001b[1;32m   3108\u001b[0m     batch_text_or_text_pairs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(text, text_pair)) \u001b[39mif\u001b[39;00m text_pair \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m text\n\u001b[0;32m-> 3109\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_encode_plus(\n\u001b[1;32m   3110\u001b[0m         batch_text_or_text_pairs\u001b[39m=\u001b[39;49mbatch_text_or_text_pairs,\n\u001b[1;32m   3111\u001b[0m         add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m   3112\u001b[0m         padding\u001b[39m=\u001b[39;49mpadding,\n\u001b[1;32m   3113\u001b[0m         truncation\u001b[39m=\u001b[39;49mtruncation,\n\u001b[1;32m   3114\u001b[0m         max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   3115\u001b[0m         stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m   3116\u001b[0m         is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m   3117\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m   3118\u001b[0m         padding_side\u001b[39m=\u001b[39;49mpadding_side,\n\u001b[1;32m   3119\u001b[0m         return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m   3120\u001b[0m         return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m   3121\u001b[0m         return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m   3122\u001b[0m         return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m   3123\u001b[0m         return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m   3124\u001b[0m         return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m   3125\u001b[0m         return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m   3126\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   3127\u001b[0m         split_special_tokens\u001b[39m=\u001b[39;49msplit_special_tokens,\n\u001b[1;32m   3128\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   3129\u001b[0m     )\n\u001b[1;32m   3130\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3131\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencode_plus(\n\u001b[1;32m   3132\u001b[0m         text\u001b[39m=\u001b[39mtext,\n\u001b[1;32m   3133\u001b[0m         text_pair\u001b[39m=\u001b[39mtext_pair,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3151\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   3152\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/Coding/NLP/NLP_basics/nlp/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3311\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   3301\u001b[0m \u001b[39m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   3302\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   3303\u001b[0m     padding\u001b[39m=\u001b[39mpadding,\n\u001b[1;32m   3304\u001b[0m     truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3308\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   3309\u001b[0m )\n\u001b[0;32m-> 3311\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_encode_plus(\n\u001b[1;32m   3312\u001b[0m     batch_text_or_text_pairs\u001b[39m=\u001b[39;49mbatch_text_or_text_pairs,\n\u001b[1;32m   3313\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m   3314\u001b[0m     padding_strategy\u001b[39m=\u001b[39;49mpadding_strategy,\n\u001b[1;32m   3315\u001b[0m     truncation_strategy\u001b[39m=\u001b[39;49mtruncation_strategy,\n\u001b[1;32m   3316\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   3317\u001b[0m     stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m   3318\u001b[0m     is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m   3319\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m   3320\u001b[0m     padding_side\u001b[39m=\u001b[39;49mpadding_side,\n\u001b[1;32m   3321\u001b[0m     return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m   3322\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m   3323\u001b[0m     return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m   3324\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m   3325\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m   3326\u001b[0m     return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m   3327\u001b[0m     return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m   3328\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   3329\u001b[0m     split_special_tokens\u001b[39m=\u001b[39;49msplit_special_tokens,\n\u001b[1;32m   3330\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   3331\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/Coding/NLP/NLP_basics/nlp/lib/python3.10/site-packages/transformers/tokenization_utils.py:888\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    886\u001b[0m     ids, pair_ids \u001b[39m=\u001b[39m ids_or_pair_ids\n\u001b[0;32m--> 888\u001b[0m first_ids \u001b[39m=\u001b[39m get_input_ids(ids)\n\u001b[1;32m    889\u001b[0m second_ids \u001b[39m=\u001b[39m get_input_ids(pair_ids) \u001b[39mif\u001b[39;00m pair_ids \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    890\u001b[0m input_ids\u001b[39m.\u001b[39mappend((first_ids, second_ids))\n",
      "File \u001b[0;32m~/Documents/Coding/NLP/NLP_basics/nlp/lib/python3.10/site-packages/transformers/tokenization_utils.py:868\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._batch_encode_plus.<locals>.get_input_ids\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    866\u001b[0m     \u001b[39mreturn\u001b[39;00m text\n\u001b[1;32m    867\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 868\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    869\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mInput is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    870\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Input is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers."
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "# Dataset\n",
    "class RagDataset(Dataset):\n",
    "    def __init__(self, questions, contexts):\n",
    "        self.questions = questions\n",
    "        self.contexts = contexts\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"question\": self.questions[idx],\n",
    "            \"context\": self.contexts[idx]\n",
    "        }\n",
    "\n",
    "# Collate to keep strings intact\n",
    "def collate_batch(batch):\n",
    "    return {\n",
    "        \"question\": [item[\"question\"] for item in batch],\n",
    "        \"context\": [item[\"context\"] for item in batch]\n",
    "    }\n",
    "\n",
    "# Loss\n",
    "def multiple_negatives_ranking_loss(q_embeds, c_embeds):\n",
    "    sim = torch.matmul(q_embeds, c_embeds.T)\n",
    "    labels = torch.arange(len(sim)).to(sim.device)\n",
    "    return F.cross_entropy(sim, labels)\n",
    "\n",
    "# Load data\n",
    "from datasets import load_dataset\n",
    "data = load_dataset(\"neural-bridge/rag-dataset-12000\", split=\"train\")\n",
    "questions, contexts = data[\"question\"], data[\"context\"]\n",
    "\n",
    "# Dataloader\n",
    "train_dataset = RagDataset(questions, contexts)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "# Model\n",
    "model = Retriever_model()\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=2e-5)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        q_texts, c_texts = batch[\"question\"], batch[\"context\"]\n",
    "        q_embed = model.encode_texts(q_texts, device)\n",
    "        c_embed = model.encode_texts(c_texts, device)\n",
    "\n",
    "        loss = multiple_negatives_ranking_loss(q_embed, c_embed)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.8f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
